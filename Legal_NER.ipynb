{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaatKdd-jyZq"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets seqeval   # seqval is used for sequece labeling tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M69NT_Ajk0VH"
      },
      "outputs": [],
      "source": [
        "#Generate training data. This training data is just a demo. It create a dataste in ConLL format\n",
        "import random\n",
        "\n",
        "names = [\"John Smith\", \"Emily Johnson\", \"Michael Brown\"]\n",
        "orgs = [\"ACME Corp\", \"Global Logistics\"]\n",
        "vehicles = [\"Toyota Camry\", \"Ford F-150\"]\n",
        "locations = [\"New York\", \"Los Angeles\"]\n",
        "dates = [\"March 5, 2022\", \"April 10, 2023\"]\n",
        "\n",
        "def format_conll(tokens, labels):\n",
        "    return \"\\n\".join(f\"{tok} {lab}\" for tok, lab in zip(tokens, labels)) + \"\\n\\n\"\n",
        "\n",
        "def annotate_report():\n",
        "    name = random.choice(names)\n",
        "    org = random.choice(orgs)\n",
        "    vehicle = random.choice(vehicles)\n",
        "    location = random.choice(locations)\n",
        "    date = random.choice(dates)\n",
        "\n",
        "    sentence = f\"On {date}, {name} was driving a {vehicle} registered to {org} near {location}.\"\n",
        "    tokens = sentence.split()\n",
        "    labels = []\n",
        "\n",
        "    for tok in tokens:\n",
        "        if date.startswith(tok):\n",
        "            labels.append(\"B-DATE\")\n",
        "        elif tok in name.split():\n",
        "            labels.append(\"B-PER\" if tok == name.split()[0] else \"I-PER\")\n",
        "        elif tok in vehicle.split():\n",
        "            labels.append(\"B-VEH\" if tok == vehicle.split()[0] else \"I-VEH\")\n",
        "        elif tok in org.split():\n",
        "            labels.append(\"B-ORG\" if tok == org.split()[0] else \"I-ORG\")\n",
        "        elif tok in location.split():\n",
        "            labels.append(\"B-GPE\")\n",
        "        else:\n",
        "            labels.append(\"O\")\n",
        "\n",
        "    return format_conll(tokens, labels)    #format tokens and labels into CoNLL fromat.\n",
        "\n",
        "with open(\"train.txt\", \"w\") as f:\n",
        "    for _ in range(100):\n",
        "        f.write(annotate_report())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert CONLL data to Huggingface dataset\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load train.txt\n",
        "def read_conll(filename):      #this function reads the data from train.txt and purses into dictionary\n",
        "    data = []\n",
        "    tokens, labels = [], []\n",
        "    with open(filename) as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                if tokens:\n",
        "                    data.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
        "                    tokens, labels = [], []\n",
        "                continue\n",
        "            token, tag = line.strip().split()\n",
        "            tokens.append(token)\n",
        "            labels.append(tag)\n",
        "    return data\n",
        "\n",
        "label_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-DATE', 'B-GPE', 'B-VEH', 'I-VEH']\n",
        "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
        "\n",
        "data = read_conll(\"train.txt\")\n",
        "for d in data:\n",
        "    d[\"ner_tags\"] = [label_to_id[tag] for tag in d[\"ner_tags\"]]\n",
        "\n",
        "dataset = Dataset.from_list(data)\n"
      ],
      "metadata": {
        "id": "cneMsIhD4TKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization and Data preparation\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n"
      ],
      "metadata": {
        "id": "-D-FfOCC92_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finetuning\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label_list))\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Create a DataCollatorForTokenClassification instance\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator # Add the data collator here\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "qlMlC8l_-A46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "\n",
        "text = \"On March 5, 2022, Emily Johnson was driving a Toyota Camry registered to ACME Corp in Los Angeles.\"\n",
        "results = ner_pipeline(text)\n",
        "\n",
        "for entity in results:\n",
        "    print(entity)"
      ],
      "metadata": {
        "id": "LvFUiTpL-LEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification # Ensure all necessary classes are imported\n",
        "\n",
        "# Define your label list (ensure it matches what you used during training)\n",
        "label_list = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-DATE', 'B-GPE', 'B-VEH', 'I-VEH']\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "\n",
        "# Associate the id2label mapping with the model's configuration\n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = {label: i for i, label in enumerate(label_list)} # It's also good practice to set label2id\n",
        "\n",
        "\n",
        "# Now create the pipeline without passing id2label directly\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "\n",
        "# Example accident report text\n",
        "text = \"On March 5, 2022, Emily Johnson was driving a Toyota Camry registered to ACME Corp in Los Angeles.\"\n",
        "results = ner_pipeline(text)\n",
        "\n",
        "# Print each detected entity with its label name\n",
        "for entity in results:\n",
        "    # The 'entity_group' key will now contain the human-readable label name\n",
        "    print(f\"Entity: {entity['word']} | Label: {entity['entity_group']} | Score: {entity['score']:.2f}\")"
      ],
      "metadata": {
        "id": "7tez_etFLhrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-qOfA6pT947"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}